{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4f94d4",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing (Descriptors + Fingerprints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from rdkit import Chem, RDLogger, DataStructs\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, MACCSkeys\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors as md\n",
    "from morfeus import XTB\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import shap\n",
    "import optuna\n",
    "from pyscf import gto, scf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6815b2a",
   "metadata": {},
   "source": [
    "## Molecule, Descriptor + Fingerprints and Outlier Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_from_smiles(smiles):\n",
    "    lg = RDLogger.logger()\n",
    "    # Temporarily silence RDKit logs\n",
    "    lg.setLevel(RDLogger.CRITICAL)\n",
    "    try:\n",
    "        # Extract molecule\n",
    "        molecule = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if molecule is None:\n",
    "            return None, \"failed\"\n",
    "\n",
    "        # Remove salts\n",
    "        clean_molecule = rdMolStandardize.LargestFragmentChooser()\n",
    "        molecule = clean_molecule.choose(molecule)\n",
    "\n",
    "        # Sanitize molecule again to reflect changes\n",
    "        Chem.SanitizeMol(molecule)\n",
    "        return molecule, \"succeed\"\n",
    "    except Exception as e:\n",
    "        return None, f\"error: {e}\"\n",
    "    finally:\n",
    "        # Re-enable logging afterward\n",
    "        lg.setLevel(RDLogger.INFO)\n",
    "\n",
    "\n",
    "def calculate_descriptors(molecule):\n",
    "    # Get all descriptors (1D/2D)\n",
    "    descriptor_names = []\n",
    "    for descriptor, _ in Descriptors._descList:\n",
    "        descriptor_names.append(descriptor)\n",
    "\n",
    "    # Use descriptors to calculate values\n",
    "    calculator = md.MolecularDescriptorCalculator(descriptor_names)\n",
    "    descriptor_values = calculator.CalcDescriptors(molecule)\n",
    "\n",
    "    # Create dictionary\n",
    "    descriptors = dict(zip(descriptor_names, descriptor_values))\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def compute_iqr_limits(df, factor=1.5):\n",
    "    # Calculate IQR limits\n",
    "    limits = {}\n",
    "    for col in df.columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # If IQR is 0 - column is too flat - skip\n",
    "        if iqr == 0 or np.isnan(iqr):\n",
    "            continue\n",
    "\n",
    "        lower = q1 - factor * iqr\n",
    "        upper = q3 + factor * iqr\n",
    "        limits[col] = (lower, upper)\n",
    "    return limits\n",
    "\n",
    "\n",
    "def apply_iqr_limits(df, limits):\n",
    "    # Apply the limits\n",
    "    df_clipped = df.copy()\n",
    "    for col, (lower, upper) in limits.items():\n",
    "        df_clipped[col] = df_clipped[col].clip(lower, upper)\n",
    "    return df_clipped\n",
    "\n",
    "\n",
    "def bitvect_to_dict(fp, prefix):\n",
    "    # Convert bit vector to dictionary (create features)\n",
    "    n_bits = fp.GetNumBits()\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    features = {}\n",
    "    for i, v in enumerate(arr):\n",
    "        features[f\"{prefix}_{i}\"] = int(v)\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_fingerprints(mol, bits=256):\n",
    "    RDLogger.DisableLog(\"rdApp.*\")\n",
    "    # Calculate Morgan, RDKit, MACCS, AtomPair and Topological Torsion fingerprint\n",
    "    feats = {}\n",
    "    if mol is None:\n",
    "        return feats\n",
    "\n",
    "    # Morgan (ECFP) fingerprint\n",
    "    morgan_bits = bits\n",
    "    morgan_radius = 2\n",
    "    fp_morgan = rdMolDescriptors.GetMorganFingerprintAsBitVect(\n",
    "        mol, radius=morgan_radius, nBits=morgan_bits\n",
    "    )\n",
    "    feats.update(bitvect_to_dict(fp_morgan, f\"Morgan{morgan_radius}_{morgan_bits}\"))\n",
    "\n",
    "    # RDKit topological fingerprint\n",
    "    rdk_bits = bits\n",
    "    fp_rdk = Chem.RDKFingerprint(mol, fpSize=rdk_bits)\n",
    "    feats.update(bitvect_to_dict(fp_rdk, f\"RDK_{rdk_bits}\"))\n",
    "\n",
    "    # MACCS keys (167 bits)\n",
    "    fp_maccs = MACCSkeys.GenMACCSKeys(mol)\n",
    "    feats.update(bitvect_to_dict(fp_maccs, \"MACCS\"))\n",
    "\n",
    "    # AtomPair fingerprint\n",
    "    ap_bits = bits\n",
    "    fp_ap = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=ap_bits)\n",
    "    feats.update(bitvect_to_dict(fp_ap, f\"AtomPair_{ap_bits}\"))\n",
    "\n",
    "    # Topological torsion fingerprint\n",
    "    tt_bits = bits\n",
    "    fp_tt = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(\n",
    "        mol, nBits=tt_bits\n",
    "    )\n",
    "    feats.update(bitvect_to_dict(fp_tt, f\"Torsion_{tt_bits}\"))\n",
    "\n",
    "    RDLogger.EnableLog(\"rdApp.*\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def prepare_3d_molecule(mol):\n",
    "    # Create 3D molecule\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    mol3d = Chem.AddHs(mol)\n",
    "\n",
    "    try:\n",
    "        # Calculate 3D coordinates and energy\n",
    "        AllChem.EmbedMolecule(mol3d, AllChem.ETKDG())\n",
    "        AllChem.UFFOptimizeMolecule(mol3d, maxIters=200)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return mol3d\n",
    "\n",
    "\n",
    "def infer_charge_and_unpaired(mol):\n",
    "    \"\"\"\n",
    "    Infer total charge and number of unpaired electrons from RDKit.\n",
    "    Good enough for typical organic molecules.\n",
    "    \"\"\"\n",
    "    total_charge = sum(a.GetFormalCharge() for a in mol.GetAtoms())\n",
    "    n_unpaired = sum(a.GetNumRadicalElectrons() for a in mol.GetAtoms())\n",
    "    return total_charge, n_unpaired\n",
    "\n",
    "\n",
    "def compute_homo_lumo_xtb(mol):\n",
    "    \"\"\"\n",
    "    Fast HOMO/LUMO computation using GFN2-xTB via morfeus.XTB.\n",
    "\n",
    "    Returns the same keys as the old PySCF version:\n",
    "        - HOMO_eV\n",
    "        - LUMO_eV\n",
    "        - HL_Gap_eV\n",
    "    \"\"\"\n",
    "    feats = {\n",
    "        \"HOMO_eV\": np.nan,\n",
    "        \"LUMO_eV\": np.nan,\n",
    "        \"HL_Gap_eV\": np.nan,\n",
    "    }\n",
    "\n",
    "    if mol is None:\n",
    "        return feats\n",
    "\n",
    "    # Create 3D molecule (reuses your existing pipeline)\n",
    "    mol3d = prepare_3d_molecule(mol)\n",
    "    if mol3d is None:\n",
    "        return feats\n",
    "\n",
    "    # Extract elements and coordinates\n",
    "    conf = mol3d.GetConformer()\n",
    "    elements = []\n",
    "    coords = []\n",
    "    for atom in mol3d.GetAtoms():\n",
    "        pos = conf.GetAtomPosition(atom.GetIdx())\n",
    "        elements.append(atom.GetSymbol())\n",
    "        coords.append([pos.x, pos.y, pos.z])\n",
    "    coords = np.array(coords, dtype=float)  # Ã…\n",
    "\n",
    "    charge, n_unpaired = infer_charge_and_unpaired(mol3d)\n",
    "\n",
    "    try:\n",
    "        # method=2 -> GFN2-xTB\n",
    "        xtb_calc = XTB(\n",
    "            elements=elements,\n",
    "            coordinates=coords,\n",
    "            method=2,\n",
    "            charge=charge,\n",
    "            n_unpaired=n_unpaired,\n",
    "        )\n",
    "\n",
    "        feats[\"HOMO_eV\"] = xtb_calc.get_homo(unit=\"eV\")\n",
    "        feats[\"LUMO_eV\"] = xtb_calc.get_lumo(unit=\"eV\")\n",
    "        feats[\"HL_Gap_eV\"] = xtb_calc.get_homo_lumo_gap(unit=\"eV\")\n",
    "\n",
    "    except Exception:\n",
    "        # keep NaNs if xtb fails\n",
    "        pass\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bbfdb",
   "metadata": {},
   "source": [
    "## Dataset Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ORIG_DATASET = \"default_datasets/in_vivo_dataset.xlsx\"\n",
    "SKIP_ROWS = 1\n",
    "SMILES_COL = \"SMILES code\"\n",
    "TARGET_COL = \"Phototoxicity\"\n",
    "FULL_OUTPUT_DATASET = \"processed_datasets/in_vivo_homolumo_processed.xlsx\"\n",
    "# Outputs\n",
    "TRAIN_X_CSV = \"prepared_datasets/in_vivo_homolumo_x_train.csv\"\n",
    "TEST_X_CSV = \"prepared_datasets/in_vivo_homolumo_x_test.csv\"\n",
    "TRAIN_Y_CSV = \"prepared_datasets/in_vivo_homolumo_y_train.csv\"\n",
    "TEST_Y_CSV = \"prepared_datasets/in_vivo_homolumo_y_test.csv\"\n",
    "\n",
    "# Near constant threshold - tolerance\n",
    "SIMILARITY_THRESHOLD = 0.9\n",
    "# Correlation threshold\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "\n",
    "# Load dataset and skip first row (Header)\n",
    "dataset = pd.read_excel(ORIG_DATASET, engine=\"openpyxl\", skiprows=SKIP_ROWS)\n",
    "\n",
    "descriptor_rows = []\n",
    "fingerprint_rows = []  # fingerprints here\n",
    "state_molecules = []\n",
    "molecules = []\n",
    "\n",
    "for smiles in dataset[SMILES_COL].astype(str):\n",
    "    molecule, state = molecule_from_smiles(smiles)\n",
    "    state_molecules.append(state)\n",
    "    molecules.append(molecule)\n",
    "\n",
    "    if molecule is None:\n",
    "        descriptor_rows.append({})\n",
    "        fingerprint_rows.append({})\n",
    "        continue\n",
    "\n",
    "    # RDKit 1D/2D\n",
    "    desc_feats = calculate_descriptors(molecule)\n",
    "\n",
    "    # HOMO / LUMO / Gap via xTB\n",
    "    homo_lumo_feats = compute_homo_lumo_xtb(molecule)\n",
    "\n",
    "    # Merge descriptors + HOMO/LUMO\n",
    "    all_feats = {**desc_feats, **homo_lumo_feats}\n",
    "    descriptor_rows.append(all_feats)\n",
    "\n",
    "    # Fingerprints (UNTOUCHED later)\n",
    "    fp_feats = calculate_fingerprints(molecule, bits=256)\n",
    "    fingerprint_rows.append(fp_feats)\n",
    "\n",
    "descriptor_data_all = pd.DataFrame(descriptor_rows)\n",
    "fingerprint_data_all = pd.DataFrame(fingerprint_rows)\n",
    "\n",
    "# Keep everything + status\n",
    "output = pd.concat(\n",
    "    [\n",
    "        dataset.reset_index(drop=True),\n",
    "        descriptor_data_all.reset_index(drop=True),\n",
    "        fingerprint_data_all.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "output[\"MoleculeStatus\"] = state_molecules\n",
    "\n",
    "# Output whole dataset with descriptors and state\n",
    "with pd.ExcelWriter(FULL_OUTPUT_DATASET, engine=\"openpyxl\") as writer:\n",
    "    output.to_excel(writer, index=False, sheet_name=\"Descriptors\")\n",
    "\n",
    "print(f\"Full - Rows: {len(output)}/Columns: {output.shape[1]}\")\n",
    "print(output.head().to_string(index=False))\n",
    "\n",
    "# Drop failed molecules - boolean array\n",
    "molecules_right = []\n",
    "for molecule in molecules:\n",
    "    molecules_right.append(molecule is not None)\n",
    "\n",
    "if not any(molecules_right):\n",
    "    raise ValueError(\"No valid molecules after SMILES parsing.\")\n",
    "\n",
    "dataset_ok = dataset.loc[molecules_right].reset_index(drop=True)\n",
    "descriptor_ok = descriptor_data_all.loc[molecules_right].reset_index(drop=True)\n",
    "fingerprint_ok = fingerprint_data_all.loc[molecules_right].reset_index(drop=True)\n",
    "\n",
    "# Target\n",
    "y_full = dataset_ok[TARGET_COL].astype(int)\n",
    "\n",
    "# Take only numeric descriptor columns\n",
    "X_full = descriptor_ok.select_dtypes(include=[np.number]).copy()\n",
    "for column in X_full.columns:\n",
    "    X_full[column] = X_full[column].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop columns that are entirely NaN\n",
    "all_nan_cols = X_full.columns[X_full.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    print(f\"Dropping {len(all_nan_cols)} NaN columns.\")\n",
    "    X_full = X_full.drop(columns=all_nan_cols)\n",
    "\n",
    "# Split dataset - train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "# Calculate medians for each column in train only\n",
    "train_medians = X_train.median(numeric_only=True)\n",
    "\n",
    "# Fill missing values in both train and test using those medians\n",
    "X_train = X_train.fillna(train_medians)\n",
    "X_test = X_test.fillna(train_medians)\n",
    "\n",
    "# Compute constants on train only\n",
    "constant_cols = []\n",
    "for col in X_train.columns:\n",
    "    top_freq = X_train[col].value_counts(normalize=True, dropna=False).max()\n",
    "    if top_freq >= SIMILARITY_THRESHOLD:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "# Drop from train and apply same drop to test\n",
    "if constant_cols:\n",
    "    X_train = X_train.drop(columns=constant_cols)\n",
    "    X_test = X_test.drop(columns=constant_cols)\n",
    "    print(f\"Dropped {len(constant_cols)} constant/almost-constant columns.\")\n",
    "\n",
    "# Compute absolute correlation matrix on training data\n",
    "corr_matrix = X_train.corr().abs()\n",
    "# Keep only upper triangle of the matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# Find columns to drop - correlation\n",
    "high_corr_cols = []\n",
    "for col in upper.columns:\n",
    "    if any(upper[col] > CORRELATION_THRESHOLD):\n",
    "        high_corr_cols.append(col)\n",
    "\n",
    "# Drop from both train and test\n",
    "if high_corr_cols:\n",
    "    X_train = X_train.drop(columns=high_corr_cols)\n",
    "    X_test = X_test.drop(columns=high_corr_cols)\n",
    "    print(f\"Dropped {len(high_corr_cols)} highly correlated columns.\")\n",
    "\n",
    "# Compute IQR limits on training data\n",
    "iqr_limits = compute_iqr_limits(X_train, factor=1.5)\n",
    "\n",
    "# Apply limits to both train and test sets\n",
    "X_train = apply_iqr_limits(X_train, iqr_limits)\n",
    "X_test = apply_iqr_limits(X_test, iqr_limits)\n",
    "\n",
    "# Preserve column names and indices so we can reconstruct DataFrames after scaling\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), columns=train_cols, index=train_index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), columns=train_cols, index=test_index\n",
    ")\n",
    "\n",
    "fps_full = fingerprint_ok.copy()\n",
    "fps_full.index = X_full.index\n",
    "\n",
    "fp_train = fps_full.loc[X_train_scaled.index]\n",
    "fp_test = fps_full.loc[X_test_scaled.index]\n",
    "\n",
    "X_train_combined = pd.concat([X_train_scaled, fp_train], axis=1)\n",
    "X_test_combined = pd.concat([X_test_scaled, fp_test], axis=1)\n",
    "\n",
    "# Save processed datasets\n",
    "X_train_scaled = X_train_combined\n",
    "X_test_scaled = X_test_combined\n",
    "\n",
    "\n",
    "X_train_scaled.to_csv(TRAIN_X_CSV, index=False)\n",
    "X_test_scaled.to_csv(TEST_X_CSV, index=False)\n",
    "y_train.to_csv(TRAIN_Y_CSV, index=False, header=[TARGET_COL])\n",
    "y_test.to_csv(TEST_Y_CSV, index=False, header=[TARGET_COL])\n",
    "\n",
    "print(f\"Train - Rows: {len(X_train_scaled)}/Columns: {X_train_scaled.shape[1]}\")\n",
    "print(\"First rows of train x:\")\n",
    "print(X_train_scaled.head().to_string(index=False))\n",
    "print(f\"Test - Rows: {len(X_test_scaled)}/Columns: {X_test_scaled.shape[1]}\")\n",
    "print(\"First rows of train y:\")\n",
    "print(y_train.head().to_string(index=False))\n",
    "X_train_scaled.describe()\n",
    "\n",
    "print(\"\\nTrain set class counts:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTrain set class ratio:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set class counts:\")\n",
    "print(y_test.value_counts())\n",
    "print(\"\\nTest set class ratio:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxipred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
