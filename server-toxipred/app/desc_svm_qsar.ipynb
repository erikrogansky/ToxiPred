{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d68f13",
   "metadata": {},
   "source": [
    "# TOXIPRED â€” SVM (Descriptors-Only) QSAR Notebook\n",
    "\n",
    "**Scope:** Rebuild 2D descriptors from raw SMILES, use scaffold-aware validation, feature selection (KBest, LASSO, SHAP, RFE/Linear SVM), tune an **RBF-SVM**, calibrate probabilities, threshold for **balanced accuracy**, define a simple **Applicability Domain (AD)**, and export metrics & artifacts with `desc_svm_*` prefixes.\n",
    "\n",
    "> **Primaries for QSAR:** We optimize **Balanced Accuracy** (widely used in regulatory QSAR). We also report **ROC-AUC**, **PR-AUC**, **F1**, **Brier** score, confusion matrix and reliability diagrams.\n",
    "\n",
    "**You may need to adjust paths and column names in the Config below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b656f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement rdkit-pypi (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for rdkit-pypi\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (from imbalanced-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (from imbalanced-learn) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/samuel/Desktop/7thSemester/ToxiPred/.venv/lib/python3.12/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: quick installs (uncomment if needed) ---\n",
    "# %pip install rdkit-pypi scikit-learn xgboost shap pandas numpy matplotlib joblib tqdm\n",
    "# %pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b993f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115029/1489634496.py:50: UserWarning: SHAP not available; SHAP-based ranking will be skipped.\n",
      "  warnings.warn(\"SHAP not available; SHAP-based ranking will be skipped.\")\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, warnings, joblib, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "    from rdkit.Chem.inchi import MolToInchiKey\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"RDKit is required. Please install 'rdkit-pypi' and restart the kernel.\"\n",
    "    ) from e\n",
    "\n",
    "# Optional (for SHAP ranking)\n",
    "_have_shap = False\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    _have_shap = True\n",
    "except Exception:\n",
    "    warnings.warn(\"SHAP not available; SHAP-based ranking will be skipped.\")\n",
    "    _have_shap = False\n",
    "\n",
    "# Optional (tree model for SHAP ranking)\n",
    "_have_xgb = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    _have_xgb = True\n",
    "except Exception:\n",
    "    warnings.warn(\"XGBoost not available; SHAP ranking will fall back or be skipped.\")\n",
    "    _have_xgb = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7bc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== CONFIGURE ========\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "PRIMARY_METRIC = (\n",
    "    \"balanced_accuracy\"  # QSAR-friendly; also report PR-AUC/ROC-AUC/F1/Brier\n",
    ")\n",
    "TEST_SIZE_FRACTION = 0.2  # external scaffold holdout ~20%\n",
    "\n",
    "# ---- File paths (adjust as needed) ----\n",
    "RAW_DATA_PATH = \"in_chemico_dataset.xlsx\"  # <== CHANGE if your file is different\n",
    "SMILES_COL = \"SMILES code\"  # <== CHANGE if your column is different\n",
    "TARGET_COL = (\n",
    "    \"Phototoxicity\"  # <== CHANGE if your column is different; 1=toxic, 0=non-toxic\n",
    ")\n",
    "\n",
    "# Output prefix\n",
    "PREFIX = \"desc_svm\"\n",
    "\n",
    "# Feature selection methods to include\n",
    "USE_METHODS = [\n",
    "    \"kbest\",\n",
    "    \"lasso\",\n",
    "    \"shap\",\n",
    "    \"rfe\",\n",
    "]  # subset to speed up, e.g., [\"kbest\",\"rfe\"]\n",
    "\n",
    "# Candidate k values for top-k selection\n",
    "TOPK_CANDIDATES = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Correlation and near-constant thresholds\n",
    "CORR_THRESH = 0.90\n",
    "TOP_VALUE_FREQ_THRESH = 0.80\n",
    "\n",
    "# IQR clipping factor\n",
    "IQR_CLIP = 3.0\n",
    "\n",
    "# SVM search space (log-uniform)\n",
    "C_RANGE = (1e-3, 1e3)\n",
    "GAMMA_RANGE = (1e-5, 1e1)\n",
    "\n",
    "N_OUTER_SPLITS = 5\n",
    "N_INNER_ITER = 30  # RandomizedSearch iterations per outer fold (adjust for speed)\n",
    "\n",
    "# KNN-AD params\n",
    "AD_K = 5\n",
    "AD_THRESH_QUANTILE = (\n",
    "    0.95  # flag as OOD if distance > 95th percentile of train distances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_fragment(smiles: str) -> Optional[Chem.Mol]:\n",
    "    # Return RDKit Mol of the largest fragment from SMILES, sanitized.\n",
    "    if not isinstance(smiles, str) or len(smiles.strip()) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "        if not frags:\n",
    "            return None\n",
    "        # choose fragment with most heavy atoms\n",
    "        mol = max(frags, key=lambda m: m.GetNumHeavyAtoms())\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "DESC_LIST = Descriptors.descList  # list of (name, function)\n",
    "\n",
    "\n",
    "def compute_descriptors(mol: Chem.Mol) -> Dict[str, float]:\n",
    "    # Compute RDKit 2D descriptors from Descriptors.descList.\n",
    "    values = {}\n",
    "    for name, func in DESC_LIST:\n",
    "        try:\n",
    "            v = func(mol)\n",
    "            if isinstance(v, float) or isinstance(v, int):\n",
    "                values[name] = float(v)\n",
    "            else:\n",
    "                values[name] = np.nan\n",
    "        except Exception:\n",
    "            values[name] = np.nan\n",
    "    return values\n",
    "\n",
    "\n",
    "def murcko_scaffold_smiles(mol: Chem.Mol) -> str:\n",
    "    try:\n",
    "        scaf = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        return Chem.MolToSmiles(scaf) if scaf is not None else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def inchi_key(mol: Chem.Mol) -> str:\n",
    "    try:\n",
    "        return MolToInchiKey(mol)\n",
    "    except Exception:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc9fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopValueFrequencyFilter(BaseEstimator, TransformerMixin):\n",
    "    # Drop columns whose most frequent value frequency >= threshold.\n",
    "    def __init__(self, threshold=0.80):\n",
    "        self.threshold = threshold\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        keep = []\n",
    "        for c in X.columns:\n",
    "            vc = X[c].value_counts(normalize=True, dropna=False)\n",
    "            top = vc.iloc[0] if not vc.empty else 1.0\n",
    "            if top < self.threshold:\n",
    "                keep.append(c)\n",
    "        self.keep_cols_ = keep\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        return X[self.keep_cols_]\n",
    "\n",
    "\n",
    "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
    "    # Drop one of highly correlated pairs (|r|>thresh) using upper-triangle scan on train.\n",
    "    def __init__(self, threshold=0.90):\n",
    "        self.threshold = threshold\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        corr = X.corr(numeric_only=True).abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        drop = [\n",
    "            column for column in upper.columns if any(upper[column] > self.threshold)\n",
    "        ]\n",
    "        self.keep_cols_ = [c for c in X.columns if c not in drop]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        return X[self.keep_cols_]\n",
    "\n",
    "\n",
    "class IQRClipper(BaseEstimator, TransformerMixin):\n",
    "    # Clip features to [Q1 - k*IQR, Q3 + k*IQR] computed on train.\n",
    "    def __init__(self, k=3.0):\n",
    "        self.k = k\n",
    "        self.bounds_ = None  # dict col -> (low, high)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        bounds = {}\n",
    "        for c in X.columns:\n",
    "            q1 = X[c].quantile(0.25)\n",
    "            q3 = X[c].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            low = q1 - self.k * iqr\n",
    "            high = q3 + self.k * iqr\n",
    "            bounds[c] = (low, high)\n",
    "        self.bounds_ = bounds\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for c, (low, high) in self.bounds_.items():\n",
    "            X[c] = X[c].clip(lower=low, upper=high)\n",
    "        return X\n",
    "\n",
    "\n",
    "class LassoSelector(BaseEstimator, TransformerMixin):\n",
    "    # Select features with non-zero coef from L1-penalized Logistic Regression.\n",
    "    def __init__(self, C=1.0, max_iter=5000):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.model_ = None\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        lr = LogisticRegression(\n",
    "            penalty=\"l1\",\n",
    "            solver=\"liblinear\",\n",
    "            class_weight=\"balanced\",\n",
    "            C=self.C,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=42,\n",
    "        )\n",
    "        lr.fit(X, y)\n",
    "        mask = np.abs(lr.coef_).sum(axis=0) > 1e-12\n",
    "        self.keep_cols_ = list(X.columns[mask])\n",
    "        self.model_ = lr\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        return X[self.keep_cols_]\n",
    "\n",
    "\n",
    "class SHAPSelector(BaseEstimator, TransformerMixin):\n",
    "    # Rank features by mean |SHAP| from a tree model (XGB), then keep top-k.\n",
    "    def __init__(self, k=64, xgb_params=None):\n",
    "        self.k = int(k)\n",
    "        self.xgb_params = xgb_params or {}\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if not (_have_shap and _have_xgb):\n",
    "            warnings.warn(\n",
    "                \"SHAPSelector skipped (missing shap/xgboost). Keeping all features.\"\n",
    "            )\n",
    "            self.keep_cols_ = list(pd.DataFrame(X).columns)\n",
    "            return self\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            max_depth=4,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            **self.xgb_params,\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        sv = explainer.shap_values(X)\n",
    "        if isinstance(sv, list):  # binary-class returns list with 2 arrays in old shap\n",
    "            sv = sv[1]\n",
    "        shap_mean = np.mean(np.abs(sv), axis=0)\n",
    "        order = np.argsort(shap_mean)[::-1]\n",
    "        cols = np.array(X.columns)[order]\n",
    "        self.keep_cols_ = list(cols[: self.k])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        return X[self.keep_cols_]\n",
    "\n",
    "\n",
    "class RFESelector(BaseEstimator, TransformerMixin):\n",
    "    # RFE using a LinearSVC (balanced). Keep exactly k features.\n",
    "    def __init__(self, k=64, max_iter=5000):\n",
    "        self.k = int(k)\n",
    "        self.max_iter = max_iter\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        from sklearn.feature_selection import RFE\n",
    "\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        base = LinearSVC(\n",
    "            dual=False, class_weight=\"balanced\", max_iter=self.max_iter, random_state=42\n",
    "        )\n",
    "        rfe = RFE(base, n_features_to_select=self.k, step=0.1)\n",
    "        rfe.fit(X, y)\n",
    "        mask = rfe.support_\n",
    "        self.keep_cols_ = list(X.columns[mask])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        return X[self.keep_cols_]\n",
    "\n",
    "\n",
    "class GenericSelector(BaseEstimator, TransformerMixin):\n",
    "    # Unified selector: method in {'kbest','lasso','shap','rfe'} with parameter k/C.\n",
    "    def __init__(self, method=\"kbest\", k=64, lasso_C=1.0):\n",
    "        self.method = method\n",
    "        self.k = int(k)\n",
    "        self.lasso_C = lasso_C\n",
    "        self.selector_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        method = self.method.lower()\n",
    "        if method == \"kbest\":\n",
    "            self.selector_ = SelectKBest(score_func=f_classif, k=self.k)\n",
    "        elif method == \"lasso\":\n",
    "            self.selector_ = LassoSelector(C=self.lasso_C)\n",
    "        elif method == \"shap\":\n",
    "            self.selector_ = SHAPSelector(k=self.k)\n",
    "        elif method == \"rfe\":\n",
    "            self.selector_ = RFESelector(k=self.k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        self.selector_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.selector_.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b0a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_threshold_max_bal_acc(y_true, p):\n",
    "    thresholds = np.unique(np.concatenate([[0.0], p, [1.0]]))\n",
    "    best_t, best_ba = 0.5, -1.0\n",
    "    for t in thresholds:\n",
    "        y_hat = (p >= t).astype(int)\n",
    "        ba = balanced_accuracy_score(y_true, y_hat)\n",
    "        if ba > best_ba:\n",
    "            best_ba, best_t = ba, t\n",
    "    return float(best_t), float(best_ba)\n",
    "\n",
    "\n",
    "def compute_metric_panel(y_true, p, threshold=0.5):\n",
    "    y_hat = (p >= threshold).astype(int)\n",
    "    return {\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_hat),\n",
    "        \"roc_auc\": roc_auc_score(y_true, p) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"pr_auc\": average_precision_score(y_true, p),\n",
    "        \"f1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"brier\": brier_score_loss(y_true, p),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_hat, labels=[0, 1]).tolist(),\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "\n",
    "def knn_ad_distance(train_X, test_X, k=5):\n",
    "    # Return mean distance to kNN in train (Euclidean) as AD score; lower=more in-domain.\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k, len(train_X)), metric=\"euclidean\")\n",
    "    nbrs.fit(train_X)\n",
    "    dists, _ = nbrs.kneighbors(test_X)\n",
    "    return dists.mean(axis=1)\n",
    "\n",
    "\n",
    "def choose_ad_threshold(train_dist, quantile=0.95):\n",
    "    return float(np.quantile(train_dist, quantile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27570f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaffolds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_random_state\n\u001b[32m      6\u001b[39m rng = check_random_state(\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m unique_scaffolds = pd.Series(\u001b[43mscaffolds\u001b[49m).astype(\u001b[38;5;28mstr\u001b[39m).unique().tolist()\n\u001b[32m      8\u001b[39m rng.shuffle(unique_scaffolds)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Greedy accumulate scaffolds until ~TEST_SIZE_FRACTION\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'scaffolds' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# External scaffold holdout split\n",
    "# ===============\n",
    "\n",
    "rng = check_random_state(42)\n",
    "unique_scaffolds = pd.Series(scaffolds).astype(str).unique().tolist()\n",
    "rng.shuffle(unique_scaffolds)\n",
    "\n",
    "# Greedy accumulate scaffolds until ~TEST_SIZE_FRACTION\n",
    "scaf_to_idx = {}\n",
    "for i, s in enumerate(scaffolds):\n",
    "    scaf_to_idx.setdefault(s, []).append(i)\n",
    "\n",
    "test_scaffs = []\n",
    "test_idx = set()\n",
    "target_test_size = int(math.ceil(TEST_SIZE_FRACTION * len(df)))\n",
    "for scaf in unique_scaffolds:\n",
    "    cand = scaf_to_idx.get(scaf, [])\n",
    "    new_size = len(test_idx) + len(cand)\n",
    "    if len(test_idx) < target_test_size or new_size <= target_test_size:\n",
    "        test_scaffs.append(scaf)\n",
    "        test_idx.update(cand)\n",
    "\n",
    "mask_test = df.index.isin(sorted(list(test_idx)))\n",
    "mask_train = ~mask_test\n",
    "\n",
    "X_train_raw, X_test_raw = X.loc[mask_train].reset_index(drop=True), X.loc[\n",
    "    mask_test\n",
    "].reset_index(drop=True)\n",
    "y_train, y_test = y[mask_train], y[mask_test]\n",
    "groups_train = pd.Series(scaffolds)[mask_train].astype(str).values\n",
    "groups_test = pd.Series(scaffolds)[mask_test].astype(str).values\n",
    "\n",
    "print(f\"Train: {len(X_train_raw)} | Test (scaffold holdout): {len(X_test_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"topval\", TopValueFrequencyFilter(threshold=TOP_VALUE_FREQ_THRESH)),\n",
    "        (\"corr\", CorrelationFilter(threshold=CORR_THRESH)),\n",
    "        (\"iqr\", IQRClipper(k=IQR_CLIP)),\n",
    "        (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on train, apply to both\n",
    "X_train_p = preproc.fit_transform(X_train_raw, y_train)\n",
    "X_test_p = preproc.transform(X_test_raw)\n",
    "\n",
    "# Keep fitted feature names after drops\n",
    "keep_cols = preproc.named_steps[\"corr\"].keep_cols_\n",
    "X_train = pd.DataFrame(X_train_p, columns=keep_cols)\n",
    "X_test = pd.DataFrame(X_test_p, columns=keep_cols)\n",
    "\n",
    "print(f\"After preprocessing: d={X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Nested CV: outer GroupKFold, inner RandomizedSearch\n",
    "# ===============\n",
    "def loguniform(low, high, size=None, rng=None):\n",
    "    rng = check_random_state(rng)\n",
    "    return np.exp(rng.uniform(np.log(low), np.log(high), size))\n",
    "\n",
    "\n",
    "outer_cv = GroupKFold(n_splits=5)\n",
    "results = []\n",
    "best_pipelines = []\n",
    "\n",
    "fold_id = 0\n",
    "for train_idx, val_idx in outer_cv.split(X_train, y_train, groups=groups_train):\n",
    "    fold_id += 1\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    g_tr, g_val = groups_train[train_idx], groups_train[val_idx]\n",
    "\n",
    "    # Pipeline: selector -> SVM\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"selector\", GenericSelector(method=\"kbest\", k=64, lasso_C=1.0)),\n",
    "            (\n",
    "                \"svm\",\n",
    "                SVC(\n",
    "                    kernel=\"rbf\",\n",
    "                    class_weight=\"balanced\",\n",
    "                    probability=True,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Randomized search space\n",
    "    space = {\n",
    "        \"selector__method\": USE_METHODS,\n",
    "        \"selector__k\": TOPK_CANDIDATES,\n",
    "        \"selector__lasso_C\": loguniform(\n",
    "            1e-3, 1e3, size=30, rng=42\n",
    "        ).tolist(),  # will be sampled below\n",
    "        \"svm__C\": loguniform(1e-3, 1e3, size=30, rng=42).tolist(),\n",
    "        \"svm__gamma\": loguniform(1e-5, 1e1, size=30, rng=42).tolist(),\n",
    "    }\n",
    "\n",
    "    # Build param distributions list (one per iteration) to ensure lasso_C is sampled\n",
    "    param_distributions = []\n",
    "    for i in range(30):\n",
    "        param_distributions.append(\n",
    "            {\n",
    "                \"selector__method\": np.random.choice(USE_METHODS),\n",
    "                \"selector__k\": int(np.random.choice(TOPK_CANDIDATES)),\n",
    "                \"selector__lasso_C\": float(space[\"selector__lasso_C\"][i]),\n",
    "                \"svm__C\": float(space[\"svm__C\"][i]),\n",
    "                \"svm__gamma\": float(space[\"svm__gamma\"][i]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Custom random search loop (group-aware)\n",
    "    best_score = -1.0\n",
    "    best_est = None\n",
    "    inner_cv = GroupKFold(n_splits=max(3, min(5, len(np.unique(g_tr)))))\n",
    "    for params in tqdm(param_distributions, desc=f\"Outer fold {fold_id} inner search\"):\n",
    "        est = clone(pipe).set_params(**params)\n",
    "        # group-aware CV score\n",
    "        scores = []\n",
    "        for it_tr_idx, it_va_idx in inner_cv.split(X_tr, y_tr, groups=g_tr):\n",
    "            Xt_tr, Xt_va = X_tr.iloc[it_tr_idx], X_tr.iloc[it_va_idx]\n",
    "            yt_tr, yt_va = y_tr[it_tr_idx], y_tr[it_va_idx]\n",
    "\n",
    "            est.fit(Xt_tr, yt_tr)\n",
    "            p_va = est.predict_proba(Xt_va)[:, 1]\n",
    "            y_hat = (p_va >= 0.5).astype(int)  # default threshold for inner scoring\n",
    "            sc = balanced_accuracy_score(yt_va, y_hat)\n",
    "            scores.append(sc)\n",
    "        score = float(np.mean(scores))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_est = est\n",
    "\n",
    "    # Refit best on outer-train\n",
    "    best_est.fit(X_tr, y_tr)\n",
    "    # Calibrate (CV=5, isotonic). Note: groups not honored in sklearn's calibrator.\n",
    "    calibrator = CalibratedClassifierCV(best_est, method=\"isotonic\", cv=5)\n",
    "    calibrator.fit(X_tr, y_tr)\n",
    "\n",
    "    # Threshold tuning on outer-train via calibration predictions\n",
    "    p_tr = calibrator.predict_proba(X_tr)[:, 1]\n",
    "    t_star, _ = pick_threshold_max_bal_acc(y_tr, p_tr)\n",
    "\n",
    "    # Evaluate on outer-val\n",
    "    p_val = calibrator.predict_proba(X_val)[:, 1]\n",
    "    panel = compute_metric_panel(y_true=y_val, p=p_val, threshold=t_star)\n",
    "    panel[\"fold\"] = fold_id\n",
    "    try:\n",
    "        panel[\"best_params\"] = {\n",
    "            \"selector__method\": calibrator.base_estimator.named_steps[\n",
    "                \"selector\"\n",
    "            ].method,\n",
    "            \"selector__k\": calibrator.base_estimator.named_steps[\"selector\"].k,\n",
    "            \"svm__C\": calibrator.base_estimator.named_steps[\"svm\"].C,\n",
    "            \"svm__gamma\": calibrator.base_estimator.named_steps[\"svm\"].gamma,\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "    results.append(panel)\n",
    "    best_pipelines.append(calibrator)\n",
    "\n",
    "# Aggregate outer-CV\n",
    "import os, json\n",
    "\n",
    "cv_df = pd.DataFrame(results)\n",
    "print(\n",
    "    cv_df[\n",
    "        [\"fold\", \"balanced_accuracy\", \"roc_auc\", \"pr_auc\", \"f1\", \"brier\", \"threshold\"]\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    \"\\nCV means:\\n\",\n",
    "    cv_df[[\"balanced_accuracy\", \"roc_auc\", \"pr_auc\", \"f1\", \"brier\"]].mean(),\n",
    ")\n",
    "\n",
    "# Save CV results\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "cv_df.to_csv(f\"artifacts/{PREFIX}_outer_cv_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best fold estimator by balanced accuracy mean surrogate\n",
    "best_idx = int(np.argmax(cv_df[\"balanced_accuracy\"].values))\n",
    "best_model = best_pipelines[best_idx]\n",
    "\n",
    "# Threshold tuning on full train\n",
    "p_train_full = best_model.predict_proba(X_train)[:, 1]\n",
    "t_star_full, _ = pick_threshold_max_bal_acc(y_train, p_train_full)\n",
    "\n",
    "# External test evaluation\n",
    "p_test = best_model.predict_proba(X_test)[:, 1]\n",
    "panel_test = compute_metric_panel(y_true=y_test, p=p_test, threshold=t_star_full)\n",
    "print(\"\\nExternal test panel:\", json.dumps(panel_test, indent=2))\n",
    "\n",
    "# Applicability Domain (AD): compute on scaled, selected features\n",
    "# Recompute transformed matrices explicitly\n",
    "prep_sel = best_model.base_estimator.named_steps[\"selector\"]\n",
    "pre_svm_cols = getattr(prep_sel.selector_, \"keep_cols_\", None)\n",
    "Xtr_sel = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "Xte_sel = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "if pre_svm_cols is not None:\n",
    "    Xtr_sel = Xtr_sel[pre_svm_cols]\n",
    "    Xte_sel = Xte_sel[pre_svm_cols]\n",
    "\n",
    "train_dist = knn_ad_distance(Xtr_sel.values, Xtr_sel.values, k=5)\n",
    "test_dist = knn_ad_distance(Xtr_sel.values, Xte_sel.values, k=5)\n",
    "ad_thresh = choose_ad_threshold(train_dist, quantile=0.95)\n",
    "ad_flag_test = (test_dist > ad_thresh).astype(int)\n",
    "\n",
    "# Save AD info\n",
    "ad_df = pd.DataFrame(\n",
    "    {\"ad_distance\": test_dist, \"ad_flag\": ad_flag_test, \"y_true\": y_test, \"p\": p_test}\n",
    ")\n",
    "ad_df.to_csv(f\"artifacts/{PREFIX}_test_ad.csv\", index=False)\n",
    "\n",
    "# Reliability diagram\n",
    "prob_true, prob_pred = [], []\n",
    "bins = np.linspace(0, 1, 11)\n",
    "inds = np.digitize(p_test, bins) - 1\n",
    "for b in range(len(bins) - 1):\n",
    "    mask = inds == b\n",
    "    if mask.any():\n",
    "        prob_true.append(np.mean(y_test[mask]))\n",
    "        prob_pred.append(np.mean(p_test[mask]))\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], \"--\", label=\"Perfect\")\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Calibrated SVM\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Empirical frequency\")\n",
    "plt.title(\"Reliability diagram (external test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"artifacts/{PREFIX}_reliability.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# PR & ROC curves\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, p_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, p_test)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR curve (external test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"artifacts/{PREFIX}_pr_curve.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC curve (external test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"artifacts/{PREFIX}_roc_curve.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Save metrics & model\n",
    "with open(f\"artifacts/{PREFIX}_test_metrics.json\", \"w\") as f:\n",
    "    json.dump(panel_test, f, indent=2)\n",
    "\n",
    "joblib.dump(best_model, f\"artifacts/{PREFIX}_best_model.joblib\")\n",
    "\n",
    "# Save processed splits for reproducibility\n",
    "X_train.to_csv(f\"artifacts/{PREFIX}_X_train.csv\", index=False)\n",
    "pd.Series(y_train).to_csv(f\"artifacts/{PREFIX}_y_train.csv\", index=False, header=[\"y\"])\n",
    "X_test.to_csv(f\"artifacts/{PREFIX}_X_test.csv\", index=False)\n",
    "pd.Series(y_test).to_csv(f\"artifacts/{PREFIX}_y_test.csv\", index=False, header=[\"y\"])\n",
    "\n",
    "print(\"Artifacts saved under ./artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5f1aa",
   "metadata": {},
   "source": [
    "## Notes & Adjustments\n",
    "\n",
    "- **Columns & paths:** Update `RAW_DATA_PATH`, `SMILES_COL`, `TARGET_COL` in the Config.\n",
    "- **SHAP ranking:** requires `shap` + `xgboost`. If not installed, the selector gracefully keeps all features or you can drop `\"shap\"` from `USE_METHODS`.\n",
    "- **Calibration:** we use isotonic CV=5. `CalibratedClassifierCV` is not group-aware; for strict group calibration, replace with group-based CV & per-fold isotonic fits.\n",
    "- **Thresholding:** chosen to maximize **Balanced Accuracy** on the (outer) training data; applied to validation/test.\n",
    "- **Applicability Domain:** simple kNN in descriptor space (post-scaling/selection). Adjust `AD_K` and `AD_THRESH_QUANTILE` as needed.\n",
    "- **Performance reporting:** See `artifacts/desc_svm_test_metrics.json` (external test) and `artifacts/desc_svm_outer_cv_metrics.csv` (outer-CV per fold). Reliability and curves are saved as PNGs.\n",
    "- **Reproducibility:** `random_state=42` applied broadly; nested CV uses **scaffold groups**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
